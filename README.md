# Akamai DS2 Dataflow

## 1. Introduction
Customers want to access, analyze and process their logs in real-time to get insights and take actions in a fast manner.

What if we could provide an easier, reliable and scalable way to collect this data?

That's what you'll get here!

## 2. Maintainers
- [Felipe Vilarinho](https://www.linkedin.com/in/fvilarinho)

If you want to collaborate in this project, reach out us by e-Mail.

You can also fork and customize this project by yourself once it's opensource. Follow the requirements below to set up 
your build environment.

## 3. Architecture
Follow this [diagram](https://viewer.diagrams.net/?tags=%7B%7D&lightbox=1&highlight=0000ff&layers=1&nav=1&title=software_architecture.drawio#R%3Cmxfile%3E%3Cdiagram%20name%3D%22Page-1%22%20id%3D%2292RJ4OFCEY1Zh97tm0ry%22%3E7Vxbc5s6EP41nmkfnAEEOH60nTjNadKmJ6dz2kfZlm0aQFSI2O6vr4SFuUi2yRSISZ2HDLoA0n7ab1e7wh0w8tY3BAbLezxDbsfQZusOuOoYRt8y2X9esdlW6MDWtjUL4sxEXVrx6PxCojLpFjkzFOY6Uoxd6gT5yin2fTSluTpICF7lu82xm39rABdIqnicQleu%2Fd%2BZ0aWo1e1%2B2vABOYulePWl0ds2eDDpLGYSLuEMrzJV4LoDRgRjur3y1iPkcuElctneN97TuhsYQT4tc4MpxvEM3UhMTgyMbpLZEhz5M8Rv0DtguFo6FD0GcMpbVwxfVrekniua5QGIMT0jQtE6UyUGdIOwhyjZsC6i1bDEoMTy6IniKhW1nYx7mZGymQgVCngXu0enEmAXQgh7JGhK80czBr4oYkKXeIF96F6ntcNUQhorpX3uMA6EXH4gSjdiJcOI4rzU0Nqh3%2FjtF5Yofc%2B0XK3Fk%2BPCRhRCSvATGmEXk3icYDy%2B1DTtEAYhjshUzGo870Z0eP%2FB0ibRl3%2Bvv%2Fmf5343USZIFoge6CcWCZfMQUQJciF1nvNq8yfwHBp1Zg0PnqAHnY5hQ4%2BvT38SBrFcbJfNajhzntnlgl9eQQqZIBH02AOMpAMbRabPqyuE2c8rRNcAskrohkIl7Co0AvzNGmGU1AjwWhoBXoewfDbMDD68%2BD3BhBdShOLSXogYPtp4XDVEiX6cCEEZEkE9ELzeKPjo3aeb20%2Ff3reEiPoN0lC%2FLSxUmWpUwF6gJHuZJ6UuQFKXW3%2FCoVQpzJh18ulVW1RGv2xQZ3oHpt9yu2yWXNl21Ss7vnVACNxkOgTY8WmYefIDr0hXgVXY0lhAK%2BC4fWKK6m5o5Xw0edfWIDvqmZWgXfSskvSYI0dx2wF%2BnGOf5hyHfl%2Fb41KMRrbdyOqyToo3TYk3v0QoQqGKNgeM9JaI9f4I50%2BwLezZqMeh69L8m3Q5ckpllFQqPa9UxikpldVKZ8SSlGqE%2FXgFE5Ve%2FQOfIev%2BGBDHXwx5HM%2FQBkFwqgrWM19RwQ4Z7Iy8H0F3hL3AdSA3qZLIP09%2B8BgvkznFhAduT1PSRSqz9CapTF7FS0qDUJIKmwzNTx26zsJn11M2dbbkwZBP2ZlCdyAaPGc221IfCp1fcBI%2FinOIcIrYc61hx7riz2JsF26JT5dIxsc%2Bf8rccd1iVZ6kjnuP5VGxtML6tyRQLAUmoApM5JXeZUUmcOhPuWWmsX2mBM7nzjRZ1ROSrGjeeRCxTj6HgyJ1j6GLp0%2F8rT57D3s1p82fzCegL8HeRXP65pC3CxszhTqqoK9EHXtndSyljjsEmtDHS6U%2Bikid0EZm1JnuONg%2FQ2Udh0oHdWHVV2I1xmQFyUyFlsb8bLHZ6fCML%2BFVyAuYyxDPKkychzOseYNoyrACvSZYE%2FcnAyudnmlR9lIUoNRFi7vRppiMHTfe%2FbwJTtxteSvAyU78bIETAIbMiSofvxKgDAkoviPiCGHf5aOcx7ghzo8uXrRTrSqFS8vDZfSMC9n%2F11VnTSoB7FJCoIH4UkGeo5GmqSM%2Bu5Z9Ca1drGlPbGkvQtlIkVE2KlRPIP94pL5fQHo7UHFXAeyXBe3N14kv7om%2FH4sU7s%2FcqyOWoPDXORi%2FZIUHRBwmPU4zf7Z8QK%2BWpfLSnI9ZWEnJmbVxyf7d%2FNG9P84RGXIMaDCl3BEuLMJwCQN%2BGXnutkNK%2B3dwgtwHRuOxuQdXE0wp9hR2geLqI2gWyNO1bShc05p27IYcrBkjOl3unCDJtrJRUd78CNi%2F0d1tK63tzgZUAV%2BRWRWZcEOry9bKG8b2RlwqRUU7jkpdWwvwRgxgKUuVCOx0019A3n0r0i1JSln7ervvfG0x83Is%2Bxw%2F7FQzZMZlYe%2FdZIYMyEeh2ktbx%2FXlBdm0YqC4QdZ6S7m0KjExikfdFJpSGyjqZFo%2BeP%2B1nU5YlRgB8zhGel1OGJDTXncYcpdZJD2JnMN8d%2FPf%2FftXNwLSFxiqZEdtRmBPYioK%2Bf5iEPdNPnjRCFx1Xhze%2Bxvyu11d8R2Zbte11N%2FSfqPOVIYSlrqshGlIqOwUh5mKgM1ic3pcoyuyPbWRjSl7nGeyeTnZqE7518U1pnzu98w1Sq5pMLZhyruEnXOTQ8X%2BGfGPvWM5dLfyG7AOuhmst5tn0Z44RB%2BjCSI%2BovEefORGYeYgKhvUZO%2FGWUa%2FLETlV4mKHfMBGz5NEZrRzWpgtgs79EZ1T%2FbNVEdUI5r7ful4gKRlXzUBlW9Vm4lSH%2F4RJmpPOL64r4iPA80dH8a%2FTMHPL%2FiwtWcY6jwAZKq8D%2BXPL1QAbfLusyk7YspUqFRkylgx%2Ff2RbZ4x%2FRUXcP0b%3C%2Fdiagram%3E%3C%2Fmxfile%3E) to check out the architecture.

## 4. Settings
If you want to customize the provisioning by yourself, just edit the following files in the `iac` directory:
- `main.tf`: Defines the provisioning providers.
- `variables.tf`: Defines the provisioning variables. You can also use `terraform.tfvars`.
- `compute.tf`: Defines the provisioning of the compute instances.
- `firewall.tf`: Defines the provisioning of the firewall rules.
- `kubernetes.tf`: Defines the provisioning of the Kubernetes cluster.
- `certissues.yaml`: Defines the provisioning of the stack TLS certificate.
- `namespaces.yaml`: Defines the provisioning of the stack namespaces.
- `configmaps.yaml`: Defines the provisioning of the stack settings.
- `secrets.yaml`: Defines the provisioning of the stack secrets.
- `deployments.yaml`: Defines the provisioning of the stack containers.
- `services.yaml`: Defines the provisioning of the stack exposed ports.
- `ingress.yaml`: Defines the provisioning of the stack ingress.
- `stack.yaml`: Defines the provisioning of the stack.

## 5. Requirements

### To build, package and publish
- [`JDK 17 or later`](https://www.oracle.com/java/technologies/downloads)
- [`Docker 24.x or later`](https://www.docker.com)
- `Any linux distribution with Kernel 5.x or later` or
- `MacOS Catalina or later` or
- `MS-Windows 10 or later with WSL2`
- `Dedicated machine with at least 4 CPU cores and 8 GB of RAM`

Just execute the shell script `build.sh` to start the compiling/building of the binaries and libraries. 
Then execute `package.sh` to start the packaging of the container images and `publish.sh` to publish them in the 
repository.

The following variables must be set in your build environment file `.env`, located in the root directory of the project.

- `DOCKER_REGISTRY_URL`: Define the docker registry repository URL to store the container images. (For example, to use 
[Docker HUB](https://hub.docker.com), the value will be `docker.io`. To use [GitHub Packages]('https://github.com'), the value will be `ghcr.io`. Please 
check the instructions of your docker registry repository).
- `DOCKER_REGISTRY_ID`: Define the docker registry repository identifier (Usually it's the username, but check the 
instructions of your docker registry repository).
- `BUILD_VERSION`: Define the version of the container images.

The following environment variables must be set in your operating system or in your CI/CD pipeline secrets.
- `DOCKER_REGISTRY_PASSWORD`: Define the rocker registry repository password.

#### Latest build status
- [![CI/CD Pipeline](https://github.com/fvilarinho/akamai-ds2-dataflow/actions/workflows/pipeline.yml/badge.svg)](https://github.com/fvilarinho/akamai-ds2-dataflow/actions/workflows/pipeline.yml)

### To deploy
Just execute the shell script `deploy.sh` (after the setup) to start the provisioning, and execute `undeploy.sh` for
de-provisioning.

After the provisioning, just execute the following commands:
- `export KUBECONFIG=iac/.kubeconfig` to specify how you'll connect in the Kubernetes cluster.
- `kubectl get nodes -o wide` to list the Kubernetes cluster nodes.
- `kubectl get pods -n akamai-ds2-dataflow -o wide` to get the details of stack pods.

To access the stack UI (after all pods started), get the IPs by executing the command `kubectl get service traefik -n akamai-siem-connector -o jsonpath='{.status.loadBalancer.ingress}`. 
Then just open your browser and type the URL: `https://<ip>/panel` and a login prompt will appear. Just type the 
credentials defined in the variables file.

To configure the ingestion in Akamai Datastream 2, use the URL: `https://<ip>/ingest` with basic authentication, sending 
the header `Content-Type: application/json`. Use the credentials defined in the variables file.

## 6. Other resources
- [Akamai Techdocs](https://techdocs.akamai.com)
- [Akamai Connected Cloud](https://www.linode.com)

And that's it! Have fun!